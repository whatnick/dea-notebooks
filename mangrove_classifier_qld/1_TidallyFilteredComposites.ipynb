{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tidally Filtered Satellite Composites\n",
    "\n",
    "### Background\n",
    "The intertidal zones across Australia possess significant environmental value, and provide a host of ecosystem services that are often poorly valued. However, mapping the extent of the major intertidal land classes (e.g. mangroves, salt marsh, clay pans) is a difficult challenge given the vast extent of Australia's coastline.  The Sentinel 2 archive in Digital Earth Australia's datacube provides a means to map the intertidal zone at scale by providing complete satellite coverage of Australia at 10m spatial resolution and a 3-5 day return time. \n",
    "\n",
    "### Description\n",
    "For mapping the extent of mangroves, salt marsh, and clay-pans in the intertidal zone we need annual composites of the mid-tide conditions to reduce noise from water inundation. This notebook will load in Sentinel 2 data from the datacube, calculate the tide height at the time of each satellite observation, and then create an image composite of only those satellite images that are within a specified tidal range. The code will also produce a number of vegetation indice summary statistics (the `Modified Soil Adjusted Vegetation Index, MSAVI`: max, min, std dev, and range), along with a map of indundation frequency, to assist in the classification script `RandomForest_classifier.ipynb`.  The workflow is as follows:  \n",
    "\n",
    "1.  Load in a Sentinel 2 time-series, and cloud mask using a prototype time-series based cloud mask.\n",
    "2.  Calculate the tide height for each satellite image. \n",
    "3.  Filter the images by the tidal range that encompasses the mid-tide height.\n",
    "4.  Calculate summary phenology statistics on `MSAVI`, and calculate the mean `Modified, Normalised Difference Water Index (MNDWI)`.\n",
    "5.  Calculate normalised inundation frequency using MNDWI.\n",
    "6.  Create annual geomedian composite images of the spectral bands. \n",
    "7.  Join the phenology summary stats and the spectral geomedians together and export the results as a netcdf file.\n",
    "8.  Export a true and false colour geotiff to assist with creating a training dataset.\n",
    "\n",
    "### Technical details\n",
    "\n",
    "* Products used: `s2a_ard_granule`, `s2b_ard_granule`\n",
    "* Analyses used: `tide modelling`, `image compositing: Geomedians` `MSAVI` `MNDWI`\n",
    "\n",
    "### Getting Started\n",
    "\n",
    "Before this notebook can run on the Sandbox, the `RF_classifier.py` script needs to be placed in the working folder:\n",
    " \n",
    "You also need to install a few libraries that aren't by default installed on the DEA Sandbox.  Run the cell below to load in the libraries (you only need to do this once, thereafter the libraries will be available). Once you've successfully run the installation, you will need to restart the kernel before the libraries will work.  Click `kernel--> Restart kernel and clear all outputs`.\n",
    "\n",
    "Delete the cell below once you've successfully installed the libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install non-default libaraies\n",
    "# !pip install --user git+https://github.com/opendatacube/datacube-stats/\n",
    "# !pip install --user hdmedians\n",
    "# !pip install --user spectral\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Once you have successful installed the non-default libraries, run this analysis by running all the cells in the notebook (shift+enter will run a highlighted cell), starting with the \"Load modules\" cell.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load datacube related modules\n",
    "import datacube\n",
    "import datacube_stats\n",
    "from datacube_stats.statistics import GeoMedian\n",
    "from datacube.storage.masking import make_mask\n",
    "import hdstats\n",
    "import odc.algo\n",
    "from datacube.utils.dask import start_local_dask\n",
    "from dask.utils import parse_bytes\n",
    "from datacube.utils.rio import configure_s3_access\n",
    "from datacube.helpers import write_geotiff\n",
    "\n",
    "#load python modules\n",
    "import warnings\n",
    "import dask\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "\n",
    "#Import DEA Notebooks scripts\n",
    "sys.path.append('../Scripts')\n",
    "import dea_datahandling\n",
    "import dea_coastaltools\n",
    "from dea_plotting import display_map\n",
    "from dea_bandindices import calculate_indices\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "#This will speed up loading data\n",
    "datacube.utils.rio.set_default_rio_config(aws='auto', cloud_defaults=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a dask local cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure dashboard link to go over proxy\n",
    "dask.config.set({\"distributed.dashboard.link\":\n",
    "                 os.environ.get('JUPYTERHUB_SERVICE_PREFIX', '/')+\"proxy/{port}/status\"});\n",
    "\n",
    "# Figure out how much memory/cpu we really have (those are set by jupyterhub)\n",
    "mem_limit = int(os.environ.get('MEM_LIMIT', '0'))\n",
    "mem_limit = mem_limit if mem_limit > 0 else parse_bytes('8Gb')\n",
    "mem_limit -= parse_bytes('2Gb') # leave 2Gb for notebook itself\n",
    "\n",
    "# close previous client if any, so that one can re-run this cell without issues\n",
    "client = locals().get('client', None)\n",
    "if client is not None:\n",
    "    client.close()\n",
    "    del client\n",
    "    \n",
    "client = start_local_dask(n_workers=1,\n",
    "                          threads_per_worker=4, \n",
    "                          memory_limit=mem_limit)\n",
    "\n",
    "# Configure GDAL for s3 access \n",
    "configure_s3_access(aws_unsigned=True,  # works only when reading public resources\n",
    "                    client=client)\n",
    "\n",
    "display(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User inputs to set up analysis\n",
    "\n",
    "The following lines allow us to set up our analysis by selecting the location (`lat_range` and `lon_range`) and time (`time_range`) we want to investigate.\n",
    "\n",
    "* **lat:** Latitude of the central point in your AOI (e.g. `-12.20`).\n",
    "\n",
    "* **lon:** Longitude of the central point in your AOI (e.g. `131.80`). \n",
    "\n",
    "* **buffer:** The distance (in decimal degrees) around your central lat/lon point. For fast load times, keep this < 0.04 degrees.\n",
    "\n",
    "* **time_range:** Enter a year to collect data from (just do one year at a time e.g. `'2018'` )\n",
    "\n",
    "* **tide_lat/lon:** The tidal model used in this analysis can only model tide heights correctly if the centre of your study area is located over open ocean. To avoid having the model fail, specify a tide modelling location that is over the open ocean nearest to the AOI you want to investigate. \n",
    "\n",
    "* **tide_range_buffer:** A buffer value (in metres, e.g `0.3`) is required to stipulate what range of tidal values should be collected to allow for image compositing of the mid-tide conditions (buffer value is centred around 0m, ie.e Mean Sea Level at that location). A greater number will include more images in the image compositing algorithm, but at the cost of intergrating greater variation of tidal conditions into the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit these lines to change any of the analysis parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = -27.651 #-27.7476  \n",
    "lon =  153.33 #153.3907\n",
    "\n",
    "buffer = 0.02\n",
    "\n",
    "time_range = '2018'\n",
    "\n",
    "tide_lat = -27.651\n",
    "tide_lon = 153.33\n",
    "\n",
    "tide_range_buffer = 0.3\n",
    "\n",
    "#set a location and name to put the results\n",
    "results = 'results/'\n",
    "savefilename = 'allSummaryStats'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the selected location\n",
    "\n",
    "The next cell will display the selected area on an interactive map. Feel free to zoom in and out to get a better understanding of the area you'll be analysing. Clicking on any point of the map will reveal the latitude and longitude coordinates of that point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_map(y=(lat-buffer, lat + buffer), x=(lon-buffer, lon + buffer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load cloud-masked Sentinel 2 data\n",
    "The first step in this analysis is to load in cloud-masked Sentinel 2 data for the `lat_range`, `lon_range` and `time_range` we provided above. We will be doing this using a protoype time-series based cloud masking approach (the algorithm looks for outliers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 'query' dictionary object, which contains the longitudes, latitudes and time provided above\n",
    "dc = datacube.Datacube(app='tidallyfilteredcomposites')\n",
    "\n",
    "query = {\n",
    "    'y':(lat-buffer, lat+buffer),\n",
    "    'x': (lon-buffer, lon+buffer),\n",
    "    'time': (time_range),\n",
    "    'dask_chunks' : {'x':200, 'y':200},\n",
    "    'output_crs': 'EPSG:3577',\n",
    "    'resolution': (-10, 10)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in the sentinel Time Series mask\n",
    "print('loading TSmask')\n",
    "tsmask_ds = dc.load(product='s2_tsmask', **query)\n",
    "\n",
    "# Load in Sentinel-2A and 2B data, just the optical bands\n",
    "bands = ['nbart_blue','nbart_green',\n",
    "         'nbart_red','nbart_red_edge_1',\n",
    "         'nbart_red_edge_2','nbart_red_edge_3',\n",
    "         'nbart_nir_1','nbart_nir_2',\n",
    "         'nbart_swir_2','nbart_swir_3']\n",
    "\n",
    "query['measurements'] = bands\n",
    "sentinel_ds = dea_datahandling.load_ard(dc=dc,\n",
    "                                        lazy_load=True,\n",
    "                                        mask_pixel_quality=False,\n",
    "                                        mask_invalid_data=False,\n",
    "                                        products=['s2a_ard_granule',\n",
    "                                                  's2b_ard_granule'],\n",
    "                                        **query)\n",
    "\n",
    "# Return only observations that have timesteps in both datasets\n",
    "matching_times = (tsmask_ds.time - sentinel_ds.time).time\n",
    "tsmask_ds = tsmask_ds.sel(time=matching_times)\n",
    "sentinel_ds = sentinel_ds.sel(time=matching_times)\n",
    "\n",
    "# mask S2 with cloud mask\n",
    "cloud_free_mask = make_mask(tsmask_ds.classification, classification='valid')\n",
    "sentinel_ds = sentinel_ds.where(cloud_free_mask)\n",
    "print(sentinel_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate band indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentinel_ds = calculate_indices(sentinel_ds, index=['MSAVI', 'MNDWI'], collection='ga_s2_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model tide heights\n",
    "As mangroves are in the inter-tidal zone, we aim to reduce the effect of tides by first modelling the tide height, and then keeping only the satellite images that were taken at the mid-tide conditions. For example, if `tide_range_buff = 0.3`, we are telling the analysis to focus only on satellite images taken when the tide was between `-0.30 m` and `+0.30 m` of the MSL.\n",
    "\n",
    "The `tidal_tag` function below uses the [OTPS TPXO9 tidal model](http://volkov.oce.orst.edu/tides/global.html) to calculate the height of the tide at the exact moment each satellite image in our dataset was taken, and adds this as a new `tide_height` attribute in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentinel_ds = dea_coastaltools.tidal_tag(\n",
    "    ds=sentinel_ds,\n",
    "    tidepost_lat=tide_lat,\n",
    "    tidepost_lon=tide_lon\n",
    ")\n",
    "sentinel_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have modelled tide heights, we can plot them to visualise the range of tide that was captured by Sentinel across our time series. In the plot below, red dashed lines also show the subset of the tidal range we selected using the `tide_range_buffer` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the resulting tide heights for each sentinel image:\n",
    "tide_range = (0-tide_range_buffer, 0+tide_range_buffer)\n",
    "\n",
    "sentinel_ds.tide_height.plot()\n",
    "plt.axhline(0, c='black', linestyle='--')\n",
    "plt.axhline(tide_range[0], c='red', linestyle='--')\n",
    "plt.axhline(tide_range[1], c='red', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Sentinel images by tide height\n",
    "Here we take the Sentinel 2 dataset and only keep the images within the tide heights given by `tide_range_buffer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentinel_filtered_all = sentinel_ds.where((sentinel_ds.tide_height > tide_range[0]) & \n",
    "                                    (sentinel_ds.tide_height < tide_range[1]), drop=True)\n",
    "\n",
    "print(\"We retained \"+ str(len(sentinel_filtered_all.time.values)) + \" images from within the nominated mid-tide range\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now compute all of the calculations we have done up to this point.\n",
    "# This will be slow\n",
    "sentinel_filtered_all = sentinel_filtered_all.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate summary phenology statistics\n",
    "\n",
    "These will help in our random forest classifier with distinguishing the different classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vegetation indice summary statistics\n",
    "msavi_mean = sentinel_filtered_all.MSAVI.mean('time').rename('msavi_mean')\n",
    "msavi_std = sentinel_filtered_all.MSAVI.std('time').rename('msavi_std')\n",
    "msavi_min = sentinel_filtered_all.MSAVI.min('time').rename('msavi_min')\n",
    "msavi_max = sentinel_filtered_all.MSAVI.max('time').rename('msavi_max')\n",
    "msavi_range = msavi_max - msavi_min\n",
    "msavi_range = msavi_range.rename('msavi_range')\n",
    "\n",
    "#and seperate out a mean of our water index\n",
    "mndwi_mean = sentinel_filtered_all.MNDWI.max('time').rename('mndwi_mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate inundation frequency\n",
    "\n",
    "This will calculate the frequency with which each pixel is inundated with water (inundation is defined as times when MNDWI indice is > 0).\n",
    "\n",
    "Frequency is defined as the number of times a pixel is observed as water divided by the number of valid observations of the pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Where MNDWI is > 0 (is water), convert to boolean\n",
    "water = sentinel_ds.MNDWI > 0\n",
    "#count the number of times we observe water per pixel\n",
    "count = water.sum(dim=['time'])\n",
    "#count number of valid observations per pixel\n",
    "valid_ds = np.isfinite(sentinel_ds.nbart_red)\n",
    "valid_sum = valid_ds.sum(dim=['time'])\n",
    "#calcuate normalised inundation frequency\n",
    "frequency = (count / valid_sum)\n",
    "frequency = frequency.rename('inundation_freq')\n",
    "frequency = frequency.compute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine observations into noise-free summary images\n",
    "Individual remote sensing images can be affected by noisy data, including clouds, sunglint and poor water quality conditions (e.g. sediment). To produce cleaner images that can be compared more easily across time, we can create 'summary' images or composites that combine multiple images into one image to reveal the median or 'typical' appearance of the landscape for a certain time period.  In the code below, we take the tidally filtered set of images and calculate an annual [Geomedian](https://github.com/daleroberts/hdmedians). A geomedian calculates a high-dimensional median value for each pixel, for each spectral band. In contrast to a standard median, a geomedian maintains the relationship between spectral bands. This allows us to conduct further analysis on the composite images just as we would on the original satellite images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GeoMedian composite\n",
    "\n",
    "This will take a couple of minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute annual geomedians from the entire mid-tide range for only the spectral bands\n",
    "sentinel_geomedian = GeoMedian().compute(sentinel_filtered_all.drop(['tide_height', 'MSAVI', 'MNDWI']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine our phenology stats with geomedians into a single object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allSummaryStats = xr.merge([sentinel_geomedian,msavi_mean,msavi_std,\n",
    "                            msavi_min,msavi_max,msavi_range,mndwi_mean,frequency])\n",
    "\n",
    "allSummaryStats.attrs = sentinel_ds.attrs\n",
    "print(allSummaryStats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export our results as a netcdf\n",
    "\n",
    "This file will be our input into the random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datacube.drivers.netcdf.write_dataset_to_netcdf(allSummaryStats, results + savefilename +'.nc')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export a true and false colour geotiff to assist with creating a training dataset\n",
    "\n",
    "When creating a training dataset, it's important to use the same imagery that will be used in classifier, rather than relying on image composites provided in google maps or esri basemaps as these will be from different time periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select just the rgb bands for the true colour geotiff\n",
    "rgb = xr.merge([sentinel_geomedian.nbart_red, sentinel_geomedian.nbart_green, sentinel_geomedian.nbart_blue])\n",
    "rgb.attrs = sentinel_geomedian.attrs\n",
    "\n",
    "#select the swir, nir, and green bands for the false colour image\n",
    "sng = xr.merge([sentinel_geomedian.nbart_swir_2, sentinel_geomedian.nbart_nir_1, sentinel_geomedian.nbart_green])\n",
    "sng.attrs = sentinel_geomedian.attrs\n",
    "\n",
    "#write out the results\n",
    "write_geotiff(results+'true_colour.tif', rgb)\n",
    "write_geotiff(results+'false_colour.tif', sng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot to check out the geomdian geotiffs look right\n",
    "b=['nbart_red','nbart_green', 'nbart_blue']\n",
    "da = rgb[b].to_array()\n",
    "img = da.plot.imshow(robust=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### That completes this script, now open `RandomForest_classifier.ipynb` to use the tidally filtered composites we just created to classify the intertidal zone in you're AOI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
