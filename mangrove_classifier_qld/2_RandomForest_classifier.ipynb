{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier\n",
    "\n",
    "### Description\n",
    "\n",
    "This script will classify the major classes of the intertidal zone (e.g. mangroves, clay-pans and salt marshes) using the annual geomedians and summary statistics produced by the script `TidallyFilteredComposites.ipynb` and a Random Forest Classifier. The workflow is as follows:\n",
    "\n",
    "1. Import training data\n",
    "2. Fit a model\n",
    "3. Classify an image\n",
    "4. Assess feature importance\n",
    "5. Assess RF parameters\n",
    "6. Export tree diagrams\n",
    "\n",
    "### Technical details\n",
    "\n",
    "* Analyses used: `Random Forest Classfication`\n",
    "\n",
    "### Getting Started\n",
    "There are a number of very important things to consider when attempting to use a Random Forest Classifier.  Consider the following before running this analysis (most of this info has been gleaned from the following review article [here](https://www.tandfonline.com/doi/full/10.1080/01431161.2018.1433343).  Some more general information about the Random Forest algorithm is available [here](https://www.kdnuggets.com/2017/10/random-forests-explained.html)\n",
    " \n",
    " - **Training data:**\n",
    "     - The most important part of any machine learning workflow. The quality of the training data has a greater impact of the classification than the algorithm used. Large and accurate training data sets are preferable: increasing the training sample size results in increased classification accuracy (Huang, Davis and Townsend 2002).\n",
    "     \n",
    "     - When creating training data, be sure to capture the spectral variability of the class. The best way to create training data is using a GIS platform. If you have access to ArcGIS Pro, there is a new tool that facilitates the easy capture of training data, and includes useful features like adjusting for class imbalance (see below). The tool collection is [here](https://pro.arcgis.com/en/pro-app/help/analysis/image-analyst/training-samples-manager.htm).\n",
    "     \n",
    "     - Random Forest (RF) classifiers are moderately robust to miss-classifications in the training data, as long as the errors are < 10-20 %\n",
    "     \n",
    "     - _Class imbalance:_ This can occurs when one of your classes is relatively rare and therefore the rare class will comprise a smaller proprotion of the training set. When imbalanced data is used, it is common that the final classification will under-predict less abundant classes relative to their true proportion. This is especially relevant to a RF classifier as the algorithm attempts to minimize error by testing overall accuracy, and the miss-classifications of rare classes do not substantially drop the overall error rate compared to optimising a very common class.\n",
    "     \n",
    "         - An approach to solving this problem can be to create a training data set, and then conduct an equalized randomly sampling strategy to select an equal number of grid cells in each class. If this approach is impossible because a class just has too few pixels, you can instead undersample the majority class, or oversample the minority class by duplicating records of the minority class.\n",
    "         \n",
    "  \n",
    " - **RF user-defined parameters:**\n",
    "   - One of the advantages of the RF classifiers is they are relatively easy to optimize.  The number of trees is the most important parameter. 100 is considered a minimum, and 500 is considered a conservative maximum.\n",
    "   - Use the `Test parameter values of RF model` section of the script to assess optimizations for other parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import RF_classifier\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from joblib import dump\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import export_graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User inputs to set up analysis\n",
    "\n",
    "* **training_shps:** These are the training data shapefiles that indicate which pixels in the satellite data will be used as training data to create the model. The training shapefile must have a 'class' attribute, where `each class is represented by a simple integer e.g mangroves = 1, saltmarsh = 2 etc.`\n",
    "\n",
    "* **train_field:** Name of the attribute column in the shapefile that contains the classes of data, classes need to be represented as integers.\n",
    "\n",
    "* **data:** This is a location string to the netcdf file we created using the script `TidallyFilteredComposites.ipynb`. This is the data that will be used to train the classifier.\n",
    "\n",
    "* **classification_output:** provide a name for the classified geotiff. eg. 'classified_image.tif'\n",
    "\n",
    "* **cpus:** The Random Forest classifier is parallel processed, so we need to specify how many cpus to run the program on. \n",
    "\n",
    "* **results:** a location path to the folder where results should be exported.\n",
    "\n",
    "* **tree_graphs:** a location path to a folder where images of the decision trees used by the RF classifier are stored.\n",
    "\n",
    "* **classification_names:** a list of the class names\n",
    "\n",
    "* **classifier_params:** change these to adjust the parameters of the RF classifier. The most important variable is `n_estimators`, this is the number of trees. See [here](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) for info on the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_shps = ['data/species_albers.shp']\n",
    "\n",
    "train_field = 'class'\n",
    "\n",
    "data = 'results/allSummaryStats.nc'\n",
    "\n",
    "classification_output = 'classified_mangroves.tif'\n",
    "\n",
    "cpus = 10\n",
    "\n",
    "results = \"results/\"\n",
    "\n",
    "tree_graphs = 'results/tree_graphs/'\n",
    "\n",
    "# Names of output classification classes\n",
    "classification_names = ['Water', 'Clay Pan', 'Salt Marsh', 'Mangrove']\n",
    "\n",
    "# Dict of RF classifier parameters\n",
    "classifier_params = {\n",
    "    'n_jobs': cpus,\n",
    "    'n_estimators': 100,\n",
    "    'criterion' : \"entropy\",\n",
    "    'verbose': False,\n",
    "    'max_features': 'auto',\n",
    "    'min_samples_leaf': 1,\n",
    "    'oob_score': True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import training data and fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract training data for each training shapefile and train classifier\n",
    "classifier, train_lab, train_samp = RF_classifier.randomforest_train(\n",
    "    train_shps=training_shps,\n",
    "    train_field=train_field,\n",
    "    xarray_data=data,\n",
    "    classifier_params=classifier_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optionally save the model\n",
    "\n",
    "Running this cell will export the RF classifier as a binary`.joblib` file. This will allow for importing the model in other scripts should we wish to use the same model to classifier a different region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(classifier, results + 'RF_Model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import analysis data and classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run classification and export to file\n",
    "analysis_xarray = xr.open_dataset(data)\n",
    "class_array, prob_array = RF_classifier.randomforest_classify(\n",
    "    classifier=classifier,\n",
    "    analysis_data=analysis_xarray,\n",
    "    classification_output=results+classification_output,\n",
    "    class_prob=True)\n",
    "\n",
    "analysis_xarray = analysis_xarray.drop('crs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the results\n",
    "\n",
    "**On the left-hand side is our classification**\n",
    "\n",
    "This plot displays the results from applying our random forest model to the input features.  The classifications are represented by the class integer we assigned in the training data.\n",
    "\n",
    "\n",
    "**One the right-hand side is a class probability map** \n",
    "\n",
    "A random forest classifier runs many decision trees (`n_estimators`). For every pixel, each decision tree in the forest makes its own decision and chooses exactly one class. The algorithm then tallies the votes of the decision trees and selects the majority class for that pixel.  A probabilty map shows the results of the voting, where the probability is determined as `n_votes / n_estimators * 100`.  This can be broadly interpeted as the confidence we have in a given classification. If a region has a low probabilty, this could mean:\n",
    "- the pixels repesent a mixed landuse zone where our classes aren't granular enough to distinguish between them.\n",
    "- that the class is unaccounted for by our training dataset (i.e. its an urban area and we don't have an urban class in the training data)\n",
    "- the training dataset is insufficient to account for a particular class' variability, in which case we may need to expand the training data.\n",
    "- the features we have included aren't useful for distinguishing between classes. e.g. maybe we need better phenology statistics to seperate two vegetation classes. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot output classification\n",
    "class_xarray = xr.DataArray(class_array,\n",
    "                            coords=[analysis_xarray.y, analysis_xarray.x],\n",
    "                            dims=['y', 'x'],\n",
    "                            name='Classification output')\n",
    "\n",
    "prob_xarray = xr.DataArray(prob_array,\n",
    "                           coords=[analysis_xarray.y, analysis_xarray.x],\n",
    "                           dims=['y', 'x'],\n",
    "                           name='Predicted class probability')\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(18, 8))\n",
    "class_xarray.plot(ax=axes[0], levels=list(np.unique(class_array))+[len(np.unique(class_array))+1],cmap=['blue', 'tan', 'coral', 'green'])\n",
    "prob_xarray.plot(ax=axes[1], cmap='plasma_r',\n",
    "                 vmin=np.percentile(prob_array, 3),\n",
    "                 vmax=np.percentile(prob_array, 97))\n",
    "\n",
    "for name, integer in zip(classification_names, np.unique(class_array)):\n",
    "    print(\"Class \" +str(int(integer))+\" = \" + name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Classifier\n",
    "\n",
    "The following cells will help you examine the classifier and improve the results.  We can do this by:\n",
    "- finding out which features (bands in the input data) are most useful for classifying, and which are not\n",
    "- by testing different input parameters to help us optimise the model\n",
    "- and by exporting the tree diagrams for each decision tree to help us understand how the RF classifier has made its decisions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine Feature Importance\n",
    "\n",
    "Extract classifier estimates of the relative importance of each band/variable for training the classifier. Useful for potentially selecting a subset of input bands/variables for model training/classification (i.e. optimising feature space). Results will be presented in descending order with the most important features listed first.  Importance is reported as a relative fraction between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the OOB accuracy\n",
    "print('The \"Out-of-Bag\" prediction of accuracy is: {oob}%'.format(oob=classifier.oob_score_ * 100))\n",
    "\n",
    "#display the relative importance of each feature\n",
    "importance = classifier.feature_importances_\n",
    "importance_df = pd.DataFrame({\n",
    "    'variable': analysis_xarray.data_vars,\n",
    "    'importance': importance})\n",
    "\n",
    "importance_df.set_index(\"variable\", inplace=True)\n",
    "display(importance_df.sort_values('importance',ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test parameter values of RF model\n",
    "\n",
    "This will help us determine which paramater values are best to use for our model. The function `GridSearchCV` will exhaustively search through the parameters in `param_grid` and determine which combination will result in the highest accuracy (based on the out-of-bag metric).  You can enter more parameters into the `param_grid` object, but be aware that the number of iterations to check will become exponentially larger as you enter more parameters. It's better practice to check only those parameters you think are likely to make a big difference to the model's accuracy. The code below will take a few minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"max_features\": ['sqrt', 'log2', 'auto'],\n",
    "    \"n_estimators\" : [100,200,500],\n",
    "    'criterion': ['entropy', 'gini'],\n",
    "    'max_depth': [1,2,5]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(classifier, param_grid, cv=3)\n",
    "grid_search.fit(train_samp, train_lab)\n",
    "\n",
    "print(\"The most accurate combination of tested parameters is: \")\n",
    "pprint(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export tree diagrams\n",
    "\n",
    "Export .png plots of each decision tree in the random forest ensemble. Useful for inspecting the splits used by the classifier to classify the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, tree_in_forest in enumerate(classifier.estimators_):    \n",
    "\n",
    "    # Create graph and save to dot file\n",
    "    export_graphviz(tree_in_forest,\n",
    "                    out_file = tree_graphs + \"tree.dot\",\n",
    "                    feature_names = list(analysis_xarray.data_vars),\n",
    "                    class_names = classification_names,\n",
    "                    filled = True,\n",
    "                    rounded = True)\n",
    "\n",
    "    # Plot as figure\n",
    "    os.system('dot -Tpng ' + tree_graphs + 'tree.dot -o ' + tree_graphs + 'tree' + str(n + 1) + '.png')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot any tree\n",
    "tree_number = 'tree5'\n",
    "\n",
    "img = plt.imread(tree_graphs + tree_number + '.png')\n",
    "plt.figure(figsize = (20, 20))\n",
    "plt.imshow(img, interpolation = \"bilinear\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
